---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## John Matthew Mason, jmm22836

### Introduction 

This dataset was acquired from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/banknote+authentication#).  They describe the data set as "data extracted from images that were taken for the evaluation of an authentication procedure for banknotes."

```{R}
library(tidyverse)

banknote = read_csv('data_banknote_authentication.csv', col_names = c('variance', 'skewness', 'curtosis', 'entropy', 'class'))

banknote %>% glimpse()

```

The dataset contains 1,372 observations of 5 variables.

|Variable Name|Description|
|---|---|
|variance|variance of Wavelet Transformed image (continuous)|
|skewness|skewness of Wavelet Transformed image (continuous)|
|curtosis|curtosis of Wavelet Transformed image (continuous)|
|entropy|entropy of image (continuous)|
|class|0 for authentic, 1 for inauthentic|

```{r}
banknote %>% group_by(class) %>% summarize(n())
```

There are 762 observations in the authentic (0) class and 610 observations in the inauthentic (1) class.

### Cluster Analysis

```{R}
library(cluster)

pam_dat <- banknote %>% select(variance, skewness, curtosis, entropy)
sil_width <- vector()

for(i in 2:10) {
  pam_fit <- pam(pam_dat, k=i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}

ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)
```

We can see that the highest silhouette width occurs at `k=2`.

```{r}
library(GGally)

pam <- pam(pam_dat, k=2)
pam_clust <- banknote %>% mutate(cluster=as.factor(pam$clustering-1))

ggpairs(pam_clust, columns=1:4, aes(color=cluster))
```

These plots show well defined boundaries between the cluster assignments, however the silhouette width suggests that the fit is not good.

```{r}
tab <- table(actual=pam_clust$class, predicted=pam_clust$cluster); tab

acc=sum(diag(tab))/sum(tab); acc
sens=tab[1,1]/rowSums(tab)[1]; sens # true positive rate
spec=tab[2,2]/rowSums(tab)[2]; spec # true negative rate
ppv=tab[1,1]/colSums(tab)[1]; ppv

fpr = 1-spec; fpr
fnr = 1-sens; fnr
```

Based on these metrics, it is clear that the PAM classifier does not do a good job of distinguishing between authentic and inauthentic checks.  

The PAM classifier has a high false positive rate of `.4` and a high false negative rate of `.36`.  For FPR and FNR, lower numbers are better.  

The PPV (the proportion of cases predicted positive that are actually positive) is low at `.67`.  For PPV, higher numbers are better.  

### Dimensionality Reduction with PCA

```{R}
pca_dat <- banknote %>% select(variance, skewness, curtosis, entropy)

pca <- princomp(pca_dat, cor=T)
summary(pca, loadings=T)

pca_res <- data.frame(PC1=pca$scores[, 1], PC2=pca$scores[,2], class=as.factor(banknote$class))
ggplot(pca_res, aes(PC1, PC2)) + geom_point(aes(color=class)) + ggtitle('Banknote Dataset PCA Decomposition')
```

The first two principle components account for a majority (`.87`) of the variance in the data set.  PC 1 increases with high variance and skewness and decreases with high curtosis and entropy.  PC 2 increases with high variance and entropy but decreases slightly with curtosis.  

In the plot, we can see some separation between the authentic (`class=0`) and inauthentic (`class=1`) checks.  However, there is still significant overlap.  

###  Linear Classifier

```{R}
log_fit <- glm(class ~ variance + skewness + curtosis + entropy, data=banknote, family='binomial')
score <- predict(log_fit, type='response')

class_diag(score, banknote$class, positive=1)

pred <- factor(score>.5,levels=c("TRUE","FALSE"))
truth <- factor(banknote$class==1, levels=c("TRUE","FALSE"))
table(truth, pred)
```

The logistic regression classifier performs very well on in-sample data.  The AUC of `.9998` is exceptional.  

```{R}
library(caret)

cv <- trainControl(method='cv', number=5, classProbs=T, savePredictions=T)
fit <- train(class ~ variance + skewness + curtosis + entropy, data=banknote, trControl=cv, method='glm')
class_diag(fit$pred$pred, fit$pred$obs, positive=1)
```

The model also performs very well when tested using cross validation.  We do not see a significant decrease in AUC, which means that our model is not overfitting the data.

### Non-Parametric Classifier

```{R}
library(caret)

knn_fit <- knn3(class ~ variance + skewness + curtosis + entropy, data=banknote, k=45)
y_hat_knn <- predict(knn_fit, banknote)
  
class_diag(y_hat_knn[,1], banknote$class, positive=0)
```
The KNN classifier performs very well on in-sample data.  The AUC of `.9998` is exceptional.    

```{R}
cv <- trainControl(method='cv', number=5, classProbs=T, savePredictions=T)
fit <- train(class ~ variance + skewness + curtosis + entropy, data=banknote, trControl=cv, method='knn', tuneGrid=expand.grid(k=45))
class_diag(fit$pred$pred, fit$pred$obs, positive=1)
```

The model also performs very well when tested using cross validation.  We do not see a significant decrease in AUC, which means that our model is not overfitting the data.


### Regression/Numeric Prediction

```{R}
fit <- lm(variance~., data=banknote)
y_hat <- predict(fit)

mean((banknote$variance - y_hat)^2)
```

The linear regression model gives a MSE of `1.25`.  The variance of the true variable is `8.08`, so a MSE of `1.25` is OK but not great.

```{R}
k=5 #choose number of folds

data <- banknote[sample(nrow(banknote)),] #randomly order rows
folds <- cut(seq(1:nrow(banknote)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  ## Fit linear regression model to training set
  fit<-lm(variance~.,data=train)
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error  (MSE) for fold i
  diags<-mean((test$variance-yhat)^2) 
}
mean(diags)
```

MSE increases somewhat when tested using cross validation, so the model shows some signs of overfitting.

### Python 

```{R}
library(reticulate)
```

```{python}
banknote_variance_mean = r.banknote['variance'].mean()
```

In Python, use the Pandas library to calculate the mean of the column `variance`.

```{r}
py$banknote_variance_mean
```

We can access the Python variable `banknote_variance_mean` in R.
